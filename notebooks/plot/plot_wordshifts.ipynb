{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "#from src.scripts.process_tweets import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through cities\n",
    "tweet_dir= Path(\"../data/processed/park_user_tweets\")\n",
    "cities = list(tweet_dir.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "# open stop words\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)\n",
    "\n",
    "# Load a cities worth of user tweets\n",
    "city = cities[10]\n",
    "# with open(city, 'r') as f:\n",
    "#    park_user_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordshift_parks(city, control_type='time'):\n",
    "    \n",
    "    # load tweets\n",
    "    with open(city, 'r') as f:\n",
    "        park_user_tweets = json.load(f)\n",
    "    print(city.name, len(park_user_tweets)) \n",
    "\n",
    "    # get control text\n",
    "    if control_type == \"time\":\n",
    "        park_tweet_text, control_tweet_text, park_list = get_time_control_text(park_user_tweets)\n",
    "    elif control_type == \"user\":\n",
    "        park_tweet_text, control_tweet_text, park_list = get_user_control_text(park_user_tweets)\n",
    "    # get stop words\n",
    "    park_stops  = get_park_stopwords(park_list)\n",
    "    stop_list = park_stops+stop_dict['all_cities']+ stop_dict[city.stem]\n",
    "    stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(stop_list), path='../src/func/ngrams.bin')).keys()]\n",
    "    \n",
    "    ## get frequency vectors\n",
    "    PVec = getVecs(park_tweet_text)\n",
    "    CVec = getVecs(control_tweet_text)\n",
    "    ## get stopped vectors and valence\n",
    "    PVec, Pval = labmtSimpleSentiment(PVec,stop_lower)\n",
    "    CVec, Cval = labmtSimpleSentiment(CVec,stop_lower)\n",
    "    \n",
    "    ## print resluts\n",
    "    print(city.stem, \": Park: {:05.3f} | Control {:05.3f}\".format(Pval, Cval))\n",
    "    labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "    \n",
    "    ## do the shift and save\n",
    "    shiftHtml(labMTvector,labMTwordList, CVec, PVec, \"shifts/\" + str(city.stem)+\"_{}.html\".format(control_type))\n",
    "    \n",
    "#for city in cities:\n",
    "#    wordshift_parks(city, 'time')\n",
    "#    wordshift_parks(city, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: ../data/processed/park_user_tweets/CO_Denver_0820000.json w 3902 park users\n",
      "City: ../data/processed/park_user_tweets/CO_Denver_0820000.json w 3902 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/AZ_Phoenix_0455000.json w 7566 park users\n",
      "City: ../data/processed/park_user_tweets/AZ_Phoenix_0455000.json w 7566 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/FL_Jacksonville_1235000.json w 3218 park users\n",
      "City: ../data/processed/park_user_tweets/FL_Jacksonville_1235000.json w 3218 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_Austin_4805000.json w 14689 park users\n",
      "City: ../data/processed/park_user_tweets/TX_Austin_4805000.json w 14689 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_Fort_Worth_4827000.json w 4278 park users\n",
      "City: ../data/processed/park_user_tweets/TX_Fort_Worth_4827000.json w 4278 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_El_Paso_4824000.json w 1397 park users\n",
      "City: ../data/processed/park_user_tweets/TX_El_Paso_4824000.json w 1397 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/CA_San_Diego_0666000.json w 22269 park users\n",
      "City: ../data/processed/park_user_tweets/CA_San_Diego_0666000.json w 22269 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TN_Memphis_4748000.json w 3112 park users\n",
      "City: ../data/processed/park_user_tweets/TN_Memphis_4748000.json w 3112 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/DC_Washington_1150000.json w 41062 park users\n",
      "City: ../data/processed/park_user_tweets/DC_Washington_1150000.json w 41062 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/WA_Seattle_5363000.json w 7739 park users\n",
      "City: ../data/processed/park_user_tweets/WA_Seattle_5363000.json w 7739 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/IL_Chicago_1714000.json w 36919 park users\n",
      "City: ../data/processed/park_user_tweets/IL_Chicago_1714000.json w 36919 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_Houston_4835000.json w 13464 park users\n",
      "City: ../data/processed/park_user_tweets/TX_Houston_4835000.json w 13464 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/OH_Cleveland_3916000.json w 3956 park users\n",
      "City: ../data/processed/park_user_tweets/OH_Cleveland_3916000.json w 3956 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/MA_Boston_2507000.json w 23479 park users\n",
      "City: ../data/processed/park_user_tweets/MA_Boston_2507000.json w 23479 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/MI_Detroit_2622000.json w 3819 park users\n",
      "City: ../data/processed/park_user_tweets/MI_Detroit_2622000.json w 3819 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/NC_Charlotte_3712000.json w 3868 park users\n",
      "City: ../data/processed/park_user_tweets/NC_Charlotte_3712000.json w 3868 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/NY_New_York_3651000.json w 113702 park users\n",
      "City: ../data/processed/park_user_tweets/NY_New_York_3651000.json w 113702 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/IN_Indianapolis_1836003.json w 5660 park users\n",
      "City: ../data/processed/park_user_tweets/IN_Indianapolis_1836003.json w 5660 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_San_Antonio_4865000.json w 12763 park users\n",
      "City: ../data/processed/park_user_tweets/TX_San_Antonio_4865000.json w 12763 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/TX_Dallas_4819000.json w 12211 park users\n",
      "City: ../data/processed/park_user_tweets/TX_Dallas_4819000.json w 12211 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/OH_Columbus_3918000.json w 4340 park users\n",
      "City: ../data/processed/park_user_tweets/OH_Columbus_3918000.json w 4340 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/PA_Philadelphia_4260000.json w 26287 park users\n",
      "City: ../data/processed/park_user_tweets/PA_Philadelphia_4260000.json w 26287 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/CA_Los_Angeles_0644000.json w 36271 park users\n",
      "City: ../data/processed/park_user_tweets/CA_Los_Angeles_0644000.json w 36271 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/MD_Baltimore_2404000.json w 5135 park users\n",
      "City: ../data/processed/park_user_tweets/MD_Baltimore_2404000.json w 5135 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/CA_San_Jose_0668000.json w 4517 park users\n",
      "City: ../data/processed/park_user_tweets/CA_San_Jose_0668000.json w 4517 park users and 1000 samples\n",
      "City: ../data/processed/park_user_tweets/CA_San_Francisco_0667000.json w 36175 park users\n",
      "City: ../data/processed/park_user_tweets/CA_San_Francisco_0667000.json w 36175 park users and 1000 samples\n"
     ]
    }
   ],
   "source": [
    "park_tweets_all = []\n",
    "control_tweets_all = []\n",
    "stop_list  = [j  for i in stop_dict.values() for j in i]\n",
    "\n",
    "for city in tweet_dir.glob(\"*.json\"):\n",
    "    with open(city, 'r') as f:\n",
    "        park_user_tweets = json.load(f)\n",
    "    park_tweet_text, control_tweet_text, park_list = get_user_control_text(park_user_tweets)\n",
    "    park_stops  = get_park_stopwords(park_list)\n",
    "    stop_list += park_stops\n",
    "    index_list = range(len(park_tweet_text))\n",
    "    n = min(1000, len(park_tweet_text))\n",
    "    sample = random.sample(index_list, n)\n",
    "    park_tweet_sample = [park_tweet_text[i] for i in sample]\n",
    "    control_tweet_sample = [control_tweet_text[i] for i in sample]\n",
    "    print(\"City: {} w {} park users and {} samples\".format(city, len(park_user_tweets),len(park_tweet_sample)))\n",
    "    park_tweets_all += park_tweet_sample\n",
    "    control_tweets_all += control_tweet_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(control_tweets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/combined_tweets/combined_tweet_sample_control_user.json\", 'w') as fp:\n",
    "    json.dump(control_tweets_all,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stops = stop_lower\n",
    "stop_dict['all_cities'].append('park')\n",
    "new_stops = stop_dict['all_cities']\n",
    "stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(new_stops), path='../src/func/ngrams.bin')).keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IL_Chicago_1714000 : Park: 6.139 | Control 6.086\n",
      "wrote shift to shifts/combined_city_user_lessstops.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## get frequency vectors\n",
    "PVec = getVecs(park_tweets_all)\n",
    "CVec = getVecs(control_tweets_all)\n",
    "## get stopped vectors and valence\n",
    "PVec, Pval = labmtSimpleSentiment(PVec,stop_lower)\n",
    "CVec, Cval = labmtSimpleSentiment(CVec,stop_lower)\n",
    "\n",
    "## print resluts\n",
    "print(city.stem, \": Park: {:05.3f} | Control {:05.3f}\".format(Pval, Cval))\n",
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "shiftHtml(labMTvector,labMTwordList, CVec, PVec, \"shifts/combined_city_user_lessstops.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"../data/processed/combined_tweets/tweet_samples_0421\")\n",
    "with open(results_dir/Path(\"park_tweets.json\"), 'r') as fp:\n",
    "    park_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"time_tweets.json\"), 'r') as fp:\n",
    "    time_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"user_tweets.json\"), 'r') as fp:\n",
    "    user_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"park_stops.json\"), 'r') as fp:\n",
    "    park_stops = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_tweet_text = []\n",
    "for city, tweets in park_tweets.items():\n",
    "    park_tweet_text += tweets\n",
    "time_tweet_text = []\n",
    "for city, tweets in time_tweets.items():\n",
    "    time_tweet_text += tweets\n",
    "stops = []\n",
    "for city, city_stops in park_stops.items():\n",
    "    stops+=city_stops\n",
    "user_tweet_text = []\n",
    "for city, tweets in user_tweets.items():\n",
    "    user_tweet_text += tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let the Black Friday madness begin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_tweet_text[10300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    }
   ],
   "source": [
    "stops = list(set([x.lower() for x in dict(regex.get_ngrams(' '.join(stops),\n",
    "                path='../src/func/ngrams.bin')).keys()]))\n",
    "\n",
    "freq_stops = []\n",
    "for city, words in stop_dict.items():\n",
    "    freq_stops+=words\n",
    "freq_stops.append('park')\n",
    "freq_stops = list(set(freq_stops))\n",
    "\n",
    "all_stops = freq_stops + stops\n",
    "\n",
    "from labMTsimple.storyLab import *\n",
    "stops_w_val=[]\n",
    "words = labmtgen.load_labmt_words(\"../data/raw/data_labmt_simple.txt\")\n",
    "senti_dict = words.set_index('Word').to_dict()['Happs']\n",
    "for stop in all_stops:\n",
    "    if stop in senti_dict.keys():\n",
    "        val = senti_dict[stop]\n",
    "        if val <= 4.0 or val >= 6.0:\n",
    "            #count+=1\n",
    "            stops_w_val.append(stop)\n",
    "            #print(stop,senti_dict[stop])\n",
    "# check how #many of stop words are in labmt outside 4-6 range\n",
    "print(len(stops_w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVec = getVecs(park_tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Happs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>park</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Happs\n",
       "518  park   7.08"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[words.Word=='park']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get frequency vectors\n",
    "PVec = getVecs(time_tweet_text)\n",
    "CVec = getVecs(user_tweet_text)\n",
    "## get stopped vectors and valence\n",
    "PVec, Pval = labmtSimpleSentiment(PVec,freq_stops)\n",
    "CVec, Cval = labmtSimpleSentiment(CVec,freq_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cities: Time control: 6.046 | User Control 6.138\n",
      "wrote shift to shifts/user_time_control_shift.html\n"
     ]
    }
   ],
   "source": [
    "## print resluts\n",
    "print(\"All cities\"\": Time control: {:05.3f} | User Control {:05.3f}\".format(Pval, Cval))\n",
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "shiftHtml(labMTvector,labMTwordList, CVec, PVec, \"shifts/user_time_control_shift.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"../data/processed/combined_tweets/tweet_samples_0513/\")\n",
    "with open(results_dir/Path(\"park_tweets_bytype.json\"), 'r') as fp:\n",
    "    park_tweets_bytype = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['acres_1_10', 'acres_gt_100', 'acres_10_100', 'acres_lt_1'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "park_tweets_bytype.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(park_tweets_bytype['acres_gt_100'])\n",
    "len(park_tweets_bytype['acres_10_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get frequency vectors\n",
    "PVec = getVecs(park_tweets_bytype['acres_gt_100'])\n",
    "CVec = getVecs(park_tweets_bytype['acres_10_100'])\n",
    "## get stopped vectors and valence\n",
    "PVec, Pval = labmtSimpleSentiment(PVec,all_stops)\n",
    "CVec, Cval = labmtSimpleSentiment(CVec,all_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cities: Park > 100: 5.939 | Park 10-100 5.897\n",
      "wrote shift to shifts/park100_park10.html\n"
     ]
    }
   ],
   "source": [
    "print(\"All cities\"\": Park > 100: {:05.3f} | Park 10-100 {:05.3f}\".format(Pval, Cval))\n",
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "shiftHtml(labMTvector,labMTwordList, CVec, PVec, \"shifts/park100_park10.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path(\"../data/processed/time_tweets/time_tweets_0424/season/\")\n",
    "with open(results_dir/Path(\"park_tweets_season_summer.json\"), 'r') as fp:\n",
    "    summer_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"park_tweets_season_winter.json\"), 'r') as fp:\n",
    "    winter_tweets = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get frequency vectors\n",
    "PVec = getVecs(summer_tweets)\n",
    "CVec = getVecs(winter_tweets)\n",
    "## get stopped vectors and valence\n",
    "PVec, Pval = labmtSimpleSentiment(PVec,all_stops)\n",
    "CVec, Cval = labmtSimpleSentiment(CVec,all_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summer: 5.960 | Winter 5.884\n",
      "wrote shift to shifts/summer_winter_inparks.html\n"
     ]
    }
   ],
   "source": [
    "print(\"Summer: {:05.3f} | Winter {:05.3f}\".format(Pval, Cval))\n",
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "shiftHtml(labMTvector,labMTwordList, CVec, PVec, \"shifts/summer_winter_inparks.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
