{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "#from src.scripts.process_tweets import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through cities\n",
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530\")\n",
    "cities = list(tweet_dir.glob(\"*.json\"))\n",
    "\n",
    "## load all stop wordds\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dict_all = stop_dict['all_cities']\n",
    "word_table = {}\n",
    "for city, words in stop_dict.items():\n",
    "    code =city[3:-8].replace(\"_\",' ')\n",
    "    words = ', '.join([x for x in words if x not in stop_dict_all])\n",
    "    if len(words) > 0:\n",
    "        word_table[code] = words\n",
    "word_table = pd.DataFrame.from_dict(word_table, 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table.to_latex('results/words/city_stops.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_tz = pd.read_csv(\"../data/processed/timezones/city_timezone.csv\", index_col='city').to_dict('dict')['tz']\n",
    "#timezones https://en.wikipedia.org/wiki/Time_in_Indiana \n",
    "#https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n",
    "eastern = \"US/Eastern\"\n",
    "pacific = \"US/Pacific\"\n",
    "arizona = \"America/Phoenix\"\n",
    "mountain = \"US/Mountain\"\n",
    "central = \"US/Central\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets_timelabel(park_user_tweets, timezone=\"US/Eastern\"):\n",
    "    tweets = get_tweets_timestamp(park_user_tweets)\n",
    "    tweet_df = pd.DataFrame.from_dict(tweets,dtype=str)\n",
    "    tweet_df['tweet_created_at'] = pd.to_datetime(tweet_df['tweet_created_at'],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    tweet_df['dt_local'] =  tweet_df['tweet_created_at'].dt.tz_localize('utc').dt.tz_convert(timezone)\n",
    "    tweet_df['hour_of_day'] = tweet_df['dt_local'].dt.hour\n",
    "    tweet_df['day_of_week'] = tweet_df['dt_local'].dt.day_name()\n",
    "    tweet_df['year'] = tweet_df['dt_local'].dt.year\n",
    "    tweet_df['season'] = tweet_df['dt_local'].dt.month.apply(lambda month:(month%12 + 3)//3)\n",
    "    tweet_df['season'].replace({1:\"winter\",2:\"spring\",3:\"summer\",4:\"fall\"}, inplace=True)\n",
    "    return tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binner='season'\n",
    "cats = ['fall', 'summer', 'winter', 'spring']\n",
    "park_tweets_bytype = {cat:[] for cat in cats}\n",
    "control_tweets_bytype = {cat:[] for cat in cats}\n",
    "park_stops = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: ../data/processed/park_user_tweets_0530/CO_Denver_0820000.json w 3902 park users\n",
      "City: ../data/processed/park_user_tweets_0530/AZ_Phoenix_0455000.json w 7566 park users\n",
      "City: ../data/processed/park_user_tweets_0530/FL_Jacksonville_1235000.json w 3218 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_Austin_4805000.json w 14689 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_Fort_Worth_4827000.json w 4278 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_El_Paso_4824000.json w 1397 park users\n",
      "City: ../data/processed/park_user_tweets_0530/CA_San_Diego_0666000.json w 22269 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TN_Memphis_4748000.json w 3112 park users\n",
      "City: ../data/processed/park_user_tweets_0530/DC_Washington_1150000.json w 41062 park users\n",
      "City: ../data/processed/park_user_tweets_0530/WA_Seattle_5363000.json w 7739 park users\n",
      "City: ../data/processed/park_user_tweets_0530/IL_Chicago_1714000.json w 36919 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_Houston_4835000.json w 13464 park users\n",
      "City: ../data/processed/park_user_tweets_0530/MA_Boston_2507000.json w 23479 park users\n",
      "City: ../data/processed/park_user_tweets_0530/MI_Detroit_2622000.json w 3819 park users\n",
      "City: ../data/processed/park_user_tweets_0530/NC_Charlotte_3712000.json w 3868 park users\n",
      "City: ../data/processed/park_user_tweets_0530/NY_New_York_3651000.json w 113702 park users\n",
      "City: ../data/processed/park_user_tweets_0530/IN_Indianapolis_1836003.json w 5660 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_San_Antonio_4865000.json w 12763 park users\n",
      "City: ../data/processed/park_user_tweets_0530/TX_Dallas_4819000.json w 12211 park users\n",
      "City: ../data/processed/park_user_tweets_0530/OH_Columbus_3918000.json w 4340 park users\n",
      "City: ../data/processed/park_user_tweets_0530/PA_Philadelphia_4260000.json w 26287 park users\n",
      "City: ../data/processed/park_user_tweets_0530/CA_Los_Angeles_0644000.json w 36271 park users\n",
      "City: ../data/processed/park_user_tweets_0530/MD_Baltimore_2404000.json w 5135 park users\n",
      "City: ../data/processed/park_user_tweets_0530/CA_San_Jose_0668000.json w 4517 park users\n",
      "City: ../data/processed/park_user_tweets_0530/CA_San_Francisco_0667000.json w 36175 park users\n"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    with open(city, 'r') as f:\n",
    "        park_user_tweets = json.load(f)\n",
    "    print(\"City: {} w {} park users\".format(city, len(park_user_tweets)))\n",
    "    tweet_df = tweets_timelabel(park_user_tweets, pacific)\n",
    "    park_list = list(set(tweet_df.park_name))\n",
    "    park_stops += get_park_stopwords(park_list)\n",
    "    for cat in cats:\n",
    "        tweet_subset = tweet_df[tweet_df[binner] ==cat]\n",
    "        index_list = range(len(tweet_subset))\n",
    "        n = min(1000, len(tweet_subset))\n",
    "        sample = random.sample(index_list, n)\n",
    "        # print(\"City: {} -{} {} tweets\".format(city, cat, n))\n",
    "        park_text = list(tweet_subset.park_text.values)\n",
    "        control_text = list(tweet_subset.control_text.values)\n",
    "        park_tweet_sample = [park_text[i] for i in sample]\n",
    "        control_tweet_sample = [control_text[i] for i in sample]\n",
    "        park_tweets_bytype[cat]+=park_tweet_sample\n",
    "        control_tweets_bytype[cat]+=control_tweet_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fall 20982\n",
      "summer 22730\n",
      "winter 21382\n",
      "spring 22898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12629"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of < 1 ..park_tweets_bytype = {cat:[] for cat in cats}\n",
    "for park_type,tweets in park_tweets_bytype.items():\n",
    "    print(park_type,len(tweets))\n",
    "\n",
    "len(park_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cats:\n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/park_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(park_tweets_bytype[cat],fp)\n",
    "    \n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/time_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(control_tweets_bytype[cat],fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969\n",
      "538\n"
     ]
    }
   ],
   "source": [
    "## load frequent stop words  stop words\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)\n",
    "    \n",
    "#load stop words selected from raw wordshifts for probelmatic words\n",
    "freq_stops = []\n",
    "for city, words in stop_dict.items():\n",
    "    freq_stops+=words\n",
    "freq_stops.append('park')\n",
    "freq_stops = list(set(freq_stops))\n",
    "\n",
    "# combine with  names of parks\n",
    "all_stops = freq_stops + park_stops\n",
    "\n",
    "\n",
    "# put through ngram filter, lower\n",
    "all_stops = list(set([x.lower() for x in dict(regex.get_ngrams(' '.join(all_stops), path='../src/func/ngrams.bin')).keys()]))\n",
    "print(len(all_stops))\n",
    "# select ones with valence below 4 or greater than 6\n",
    "stops_w_val=[]\n",
    "words = labmtgen.load_labmt_words(\"../data/raw/data_labmt_simple.txt\")\n",
    "senti_dict = words.set_index('Word').to_dict()['Happs']\n",
    "for stop in all_stops:\n",
    "    if stop in senti_dict.keys():\n",
    "        val = senti_dict[stop]\n",
    "        if val <= 4.0 or val >= 6.0:\n",
    "            #count+=1\n",
    "            stops_w_val.append(stop)\n",
    "            #print(stop,senti_dict[stop])\n",
    "# check how #many of stop words are in labmt outside 4-6 range\n",
    "print(len(stops_w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bytype ={}\n",
    "for cat in cats:\n",
    "    results_bytype[cat] =  bootstrap_sentiment(park_tweets_bytype[cat], control_tweets_bytype[cat], stops_w_val,sample=.8,runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall': [0.0892321731802026,\n",
       "  0.09273914772683067,\n",
       "  0.092336844478913,\n",
       "  0.0922857165848443,\n",
       "  0.0879960163259188,\n",
       "  0.10544741882965436,\n",
       "  0.09698224415582501,\n",
       "  0.08672222095585536,\n",
       "  0.0724846822062366,\n",
       "  0.08189164035924357],\n",
       " 'summer': [0.12061366876074153,\n",
       "  0.10920674437790634,\n",
       "  0.12444016516923462,\n",
       "  0.11935298311668507,\n",
       "  0.12262197304191247,\n",
       "  0.11088711823612396,\n",
       "  0.10928853941832184,\n",
       "  0.11260601667189274,\n",
       "  0.11398938429753525,\n",
       "  0.12380639835050644],\n",
       " 'winter': [0.03637892432251455,\n",
       "  0.04568758818766572,\n",
       "  0.052808311049188816,\n",
       "  0.04815781769147609,\n",
       "  0.042569838593917275,\n",
       "  0.04288115219313582,\n",
       "  0.04354434836902943,\n",
       "  0.03926473048051182,\n",
       "  0.04973774248473273,\n",
       "  0.04674372656678916],\n",
       " 'spring': [0.06948189766521828,\n",
       "  0.08400329145956142,\n",
       "  0.08208108563253091,\n",
       "  0.06885467183524163,\n",
       "  0.07862857773813658,\n",
       "  0.08436167099283498,\n",
       "  0.07465485681875617,\n",
       "  0.07791932808765178,\n",
       "  0.07743087155175132,\n",
       "  0.08557041289987044]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_bytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/city_combined_0605/bootstrap_{}.json\".format(binner), 'w') as fp:\n",
    "    json.dump(results_bytype,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binner='day_of_week'\n",
    "cats = ['Sunday', 'Friday', 'Monday', 'Saturday', 'Thursday', 'Wednesday', 'Tuesday']\n",
    "park_tweets_bytype = {cat:[] for cat in cats}\n",
    "control_tweets_bytype = {cat:[] for cat in cats}\n",
    "\n",
    "for city in tweet_dir.glob(\"*.json\"):\n",
    "    with open(city, 'r') as f:\n",
    "        park_user_tweets = json.load(f)\n",
    "    tweet_df = tweets_timelabel(park_user_tweets)\n",
    "    park_list = list(set(tweet_df.park_name))\n",
    "    park_stops += get_park_stopwords(park_list)\n",
    "\n",
    "    for cat in cats:\n",
    "        tweet_subset = tweet_df[tweet_df[binner] == cat]\n",
    "        index_list = range(len(tweet_subset))\n",
    "        n = min(1000, len(tweet_subset))\n",
    "        sample = random.sample(index_list, n)\n",
    "        park_text = list(tweet_subset.park_text.values)\n",
    "        control_text = list(tweet_subset.control_text.values)\n",
    "        park_tweet_sample = [park_text[i] for i in sample]\n",
    "        control_tweet_sample = [control_text[i] for i in sample]\n",
    "        park_tweets_bytype[cat]+=park_tweet_sample\n",
    "        control_tweets_bytype[cat]+=control_tweet_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday 19025\n",
      "Friday 18006\n",
      "Monday 17161\n",
      "Saturday 20124\n",
      "Thursday 17594\n",
      "Wednesday 17480\n",
      "Tuesday 17131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25244"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of < 1 ..park_tweets_bytype = {cat:[] for cat in cats}\n",
    "for park_type,tweets in park_tweets_bytype.items():\n",
    "    print(park_type,len(tweets))\n",
    "\n",
    "len(park_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cats:\n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/park_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(park_tweets_bytype[cat],fp)\n",
    "    \n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/time_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(control_tweets_bytype[cat],fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7086\n",
      "545\n"
     ]
    }
   ],
   "source": [
    "## load frequent stop words  stop words\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)\n",
    "    \n",
    "#load stop words selected from raw wordshifts for probelmatic words\n",
    "freq_stops = [\n",
    "    \n",
    "]\n",
    "for city, words in stop_dict.items():\n",
    "    freq_stops+=words\n",
    "freq_stops.append('park')\n",
    "freq_stops = list(set(freq_stops))\n",
    "\n",
    "# combine with  names of parks\n",
    "all_stops = freq_stops + park_stops\n",
    "\n",
    "\n",
    "# put through ngram filter, lower\n",
    "all_stops = list(set([x.lower() for x in dict(regex.get_ngrams(' '.join(all_stops), path='../src/func/ngrams.bin')).keys()]))\n",
    "print(len(all_stops))\n",
    "# select ones with valence below 4 or greater than 6\n",
    "stops_w_val=[]\n",
    "words = labmtgen.load_labmt_words(\"../data/raw/data_labmt_simple.txt\")\n",
    "senti_dict = words.set_index('Word').to_dict()['Happs']\n",
    "for stop in all_stops:\n",
    "    if stop in senti_dict.keys():\n",
    "        val = senti_dict[stop]\n",
    "        if val <= 4.0 or val >= 6.0:\n",
    "            #count+=1\n",
    "            stops_w_val.append(stop)\n",
    "            #print(stop,senti_dict[stop])\n",
    "# check how #many of stop words are in labmt outside 4-6 range\n",
    "print(len(stops_w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday\n",
      "Friday\n",
      "Monday\n",
      "Saturday\n",
      "Thursday\n",
      "Wednesday\n",
      "Tuesday\n"
     ]
    }
   ],
   "source": [
    "results_bytype ={}\n",
    "for cat in cats:\n",
    "    print(cat)\n",
    "    results_bytype[cat] =  bootstrap_sentiment(park_tweets_bytype[cat], control_tweets_bytype[cat], stops_w_val,sample=.8,runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'day_of_week'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/city_combined_0605/bootstrap_{}.json\".format(binner), 'w') as fp:\n",
    "    json.dump(results_bytype,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "binner='hour_of_day'\n",
    "cats = range(24)\n",
    "park_tweets_bytype = {cat:[] for cat in cats}\n",
    "control_tweets_bytype = {cat:[] for cat in cats}\n",
    "\n",
    "for city in tweet_dir.glob(\"*.json\"):\n",
    "    with open(city, 'r') as f:\n",
    "        park_user_tweets = json.load(f)\n",
    "    tweet_df = tweets_timelabel(park_user_tweets, city_tz[city.stem])\n",
    "\n",
    "    park_list = list(set(tweet_df.park_name))\n",
    "    park_stops += get_park_stopwords(park_list)\n",
    "\n",
    "    for cat in cats:\n",
    "        tweet_subset = tweet_df[tweet_df[binner] == cat]\n",
    "        index_list = range(len(tweet_subset))\n",
    "        n = min(1000, len(tweet_subset))\n",
    "        sample = random.sample(index_list, n)\n",
    "        park_text = list(tweet_subset.park_text.values)\n",
    "        control_text = list(tweet_subset.control_text.values)\n",
    "        park_tweet_sample = [park_text[i] for i in sample]\n",
    "        control_tweet_sample = [control_text[i] for i in sample]\n",
    "        park_tweets_bytype[cat]+=park_tweet_sample\n",
    "        control_tweets_bytype[cat]+=control_tweet_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for hour, tweets in park_tweets_bytype.items():\n",
    "#    print(hour,len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cats:\n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/park_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(park_tweets_bytype[cat],fp)\n",
    "    \n",
    "    with open(\"../data/processed/time_tweets/time_tweets_0605/control_tweets_{}_{}.json\".format(binner,cat), 'w') as fp:\n",
    "        json.dump(control_tweets_bytype[cat],fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7144\n",
      "545\n"
     ]
    }
   ],
   "source": [
    "## load frequent stop words  stop words\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)\n",
    "    \n",
    "#load stop words selected from raw wordshifts for probelmatic words\n",
    "freq_stops = []\n",
    "for city, words in stop_dict.items():\n",
    "    freq_stops+=words\n",
    "freq_stops.append('park')\n",
    "freq_stops = list(set(freq_stops))\n",
    "\n",
    "# combine with  names of parks\n",
    "all_stops = freq_stops + park_stops\n",
    "\n",
    "\n",
    "# put through ngram filter, lower\n",
    "all_stops = list(set([x.lower() for x in dict(regex.get_ngrams(' '.join(all_stops), path='../src/func/ngrams.bin')).keys()]))\n",
    "print(len(all_stops))\n",
    "# select ones with valence below 4 or greater than 6\n",
    "stops_w_val=[]\n",
    "words = labmtgen.load_labmt_words(\"../data/raw/data_labmt_simple.txt\")\n",
    "senti_dict = words.set_index('Word').to_dict()['Happs']\n",
    "for stop in all_stops:\n",
    "    if stop in senti_dict.keys():\n",
    "        val = senti_dict[stop]\n",
    "        if val <= 4.0 or val >= 6.0:\n",
    "            #count+=1\n",
    "            stops_w_val.append(stop)\n",
    "            #print(stop,senti_dict[stop])\n",
    "# check how #many of stop words are in labmt outside 4-6 range\n",
    "print(len(stops_w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bytype ={}\n",
    "n=5000\n",
    "\n",
    "for cat in range(8,24):\n",
    "    index_list = range(len(park_tweets_bytype[cat]))\n",
    "    size = min(len(park_tweets_bytype[cat]),10000)\n",
    "    sample = random.sample(index_list, size)\n",
    "    park_tweet_sample = [park_tweets_bytype[cat][i] for i in sample]\n",
    "    control_tweet_sample = [control_tweets_bytype[cat][i] for i in sample]\n",
    "    results_bytype[cat] =  bootstrap_sentiment(park_tweet_sample,control_tweet_sample, stops_w_val,sample=.8,runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/city_combined_0605/bootstrap_{}.json\".format(binner), 'w') as fp:\n",
    "    json.dump(results_bytype,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
