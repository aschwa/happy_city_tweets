{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "#from src.scripts.process_tweets import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through cities\n",
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530/\")\n",
    "cities = list(tweet_dir.glob(\"*.json\"))\n",
    "\n",
    "## load all stop wordds\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2649276604737896,\n",
       " 0.2703131775603964,\n",
       " 0.2762774607979246,\n",
       " 0.27388624292304176,\n",
       " 0.2730704345396955]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test time control \n",
    "park_tweet_text, control_tweet_text, park_list = get_time_control_text(park_user_tweets)\n",
    "park_stops  = get_park_stopwords(park_list)\n",
    "stop_list = park_stops+stop_dict['all_cities']+ stop_dict[city.stem]\n",
    "bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_list,sample=.8,runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17795150667741133,\n",
       " 0.18529823330290363,\n",
       " 0.18207451565484334,\n",
       " 0.1809849095152778,\n",
       " 0.1921141852609436]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test user control\n",
    "park_tweet_text, control_tweet_text, park_list = get_user_control_text(park_user_tweets)\n",
    "park_stops  = get_park_stopwords(park_list)\n",
    "stop_list = park_stops+stop_dict['all_cities']+ stop_dict[city.stem]\n",
    "bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_list,sample=.8,runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO_Denver_0820000.json 3902\n",
      "AZ_Phoenix_0455000.json 7566\n",
      "FL_Jacksonville_1235000.json 3218\n",
      "TX_Austin_4805000.json 14689\n",
      "TX_Fort_Worth_4827000.json 4278\n",
      "TX_El_Paso_4824000.json 1397\n",
      "CA_San_Diego_0666000.json 22269\n",
      "TN_Memphis_4748000.json 3112\n",
      "DC_Washington_1150000.json 41062\n",
      "WA_Seattle_5363000.json 7739\n",
      "IL_Chicago_1714000.json 36919\n",
      "TX_Houston_4835000.json 13464\n",
      "MA_Boston_2507000.json 23479\n",
      "MI_Detroit_2622000.json 3819\n",
      "NC_Charlotte_3712000.json 3868\n",
      "NY_New_York_3651000.json 113702\n",
      "IN_Indianapolis_1836003.json 5660\n",
      "TX_San_Antonio_4865000.json 12763\n",
      "TX_Dallas_4819000.json 12211\n",
      "OH_Columbus_3918000.json 4340\n",
      "PA_Philadelphia_4260000.json 26287\n",
      "CA_Los_Angeles_0644000.json 36271\n",
      "MD_Baltimore_2404000.json 5135\n",
      "CA_San_Jose_0668000.json 4517\n",
      "CA_San_Francisco_0667000.json 36175\n"
     ]
    }
   ],
   "source": [
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530/\")\n",
    "for city in tweet_dir.glob(\"*.json\"):\n",
    "    if city.name not in results_dict.keys():\n",
    "        with open(city, 'r') as f:\n",
    "            park_user_tweets = json.load(f)\n",
    "        print(city.name, len(park_user_tweets))\n",
    "        park_tweet_text, control_tweet_text, park_list = get_time_control_text(park_user_tweets)\n",
    "        park_stops  = get_park_stopwords(park_list)\n",
    "        stop_list = park_stops+stop_dict['all_cities']+ stop_dict[city.stem]\n",
    "        stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(stop_list), path='../src/func/ngrams.bin')).keys()]\n",
    "        park_sentis, control_sentis = bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_lower,sample=.8,runs=10, bumps=False)\n",
    "        results_dict[city.name]={\"park_sentiment\":park_sentis,\"control_sentiment\":control_sentis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/bootstrap_time_0530.json\", 'w') as fp:\n",
    "    json.dump(results_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_u_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO_Denver_0820000.json 3902\n",
      "AZ_Phoenix_0455000.json 7566\n",
      "FL_Jacksonville_1235000.json 3218\n",
      "TX_Austin_4805000.json 14689\n",
      "TX_Fort_Worth_4827000.json 4278\n",
      "TX_El_Paso_4824000.json 1397\n",
      "CA_San_Diego_0666000.json 22269\n",
      "TN_Memphis_4748000.json 3112\n",
      "DC_Washington_1150000.json 41062\n",
      "WA_Seattle_5363000.json 7739\n",
      "IL_Chicago_1714000.json 36919\n",
      "TX_Houston_4835000.json 13464\n",
      "MA_Boston_2507000.json 23479\n",
      "MI_Detroit_2622000.json 3819\n",
      "NC_Charlotte_3712000.json 3868\n",
      "NY_New_York_3651000.json 113702\n",
      "IN_Indianapolis_1836003.json 5660\n",
      "TX_San_Antonio_4865000.json 12763\n",
      "TX_Dallas_4819000.json 12211\n",
      "OH_Columbus_3918000.json 4340\n",
      "PA_Philadelphia_4260000.json 26287\n",
      "CA_Los_Angeles_0644000.json 36271\n",
      "MD_Baltimore_2404000.json 5135\n",
      "CA_San_Jose_0668000.json 4517\n",
      "CA_San_Francisco_0667000.json 36175\n"
     ]
    }
   ],
   "source": [
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530\")\n",
    "for city in tweet_dir.glob(\"*.json\"):\n",
    "    if city.name not in results_u_dict.keys():\n",
    "        with open(city, 'r') as f:\n",
    "            park_user_tweets = json.load(f)\n",
    "        print(city.name, len(park_user_tweets))\n",
    "        park_tweet_text, control_tweet_text, park_list = get_user_control_text(park_user_tweets)\n",
    "        park_stops  = get_park_stopwords(park_list)\n",
    "        stop_list = park_stops+stop_dict['all_cities']+ stop_dict[city.stem]\n",
    "        stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(stop_list), path='../src/func/ngrams.bin')).keys()]\n",
    "        park_sentis, control_sentis = bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_lower,sample=.8,runs=10, bumps=False)\n",
    "        results_u_dict[city.name]={\"park_sentiment\":park_sentis,\"control_sentiment\":control_sentis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/bootstrap_user_0530.json\", 'w') as fp:\n",
    "    json.dump(results_u_dict,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sensitivity tests and stop word check ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% sampled: Mean Park: 0.0173 with range 0.0625\n",
      "20% sampled: Mean Park: 0.0326 with range 0.0731\n",
      "30% sampled: Mean Park: 0.0257 with range 0.0619\n",
      "40% sampled: Mean Park: 0.0207 with range 0.0437\n",
      "50% sampled: Mean Park: 0.0255 with range 0.0370\n",
      "60% sampled: Mean Park: 0.0207 with range 0.0249\n",
      "70% sampled: Mean Park: 0.0234 with range 0.0280\n",
      "80% sampled: Mean Park: 0.0227 with range 0.0167\n",
      "90% sampled: Mean Park: 0.0226 with range 0.0040\n"
     ]
    }
   ],
   "source": [
    "# Now we bootstrap a vector of 100 seniments, by choosing 80% of the tweets in each group\n",
    "for n in np.arange(.1,1,.1):\n",
    "    bumps = bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_list,sample=n,runs=5)\n",
    "    print(\"{:.0f}% sampled: Mean Park: {:.4f} with range {:.4f}\".format(n*100, np.mean(bumps), max(bumps)-min(bumps)))\n",
    "#print(min(park_sentis), np.mean(park_sentis), max(park_sentis))\n",
    "#print(min(control_sentis), np.mean(control_sentis), max(control_sentis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(stop_list), path='../src/func/ngrams.bin')).keys()]\n",
    "for word in stop_lower:\n",
    "    if word in labMTwordList:\n",
    "        i = labMTwordList.index(word)\n",
    "        val = labMTvector[i]\n",
    "        if val < 4 or val > 6:\n",
    "            pass\n",
    "            #print(word, labMTvector[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
