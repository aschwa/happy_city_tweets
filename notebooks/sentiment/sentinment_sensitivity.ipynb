{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "#from src.scripts.process_tweets import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through cities\n",
    "tweet_dir= Path(\"../data/processed/park_user_tweets\")\n",
    "cities = list(tweet_dir.glob(\"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all stop wordds\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX_El_Paso_4824000\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "print(cities[5].stem)\n",
    "with open(cities[5], 'r') as f:\n",
    "    park_user_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test time control \n",
    "park_tweet_text, control_tweet_text, park_list = get_time_control_text(park_user_tweets)\n",
    "park_stops  = get_park_stopwords(park_list)\n",
    "stop_list = park_stops+stop_dict['all_cities']+ stop_dict[cities[5].stem]\n",
    "#bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_list,sample=.8,runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sensitivity tests and stop word check ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% sampled: Mean Park: 0.2728 with range 0.1067\n",
      "20% sampled: Mean Park: 0.2534 with range 0.0409\n",
      "30% sampled: Mean Park: 0.2628 with range 0.1100\n",
      "40% sampled: Mean Park: 0.2694 with range 0.0546\n",
      "50% sampled: Mean Park: 0.2564 with range 0.0277\n",
      "60% sampled: Mean Park: 0.2592 with range 0.0227\n",
      "70% sampled: Mean Park: 0.2654 with range 0.0103\n",
      "80% sampled: Mean Park: 0.2642 with range 0.0283\n",
      "90% sampled: Mean Park: 0.2682 with range 0.0193\n"
     ]
    }
   ],
   "source": [
    "# Now we bootstrap a vector of 100 seniments, by choosing 80% of the tweets in each group\n",
    "for n in np.arange(.1,1,.1):\n",
    "    bumps = bootstrap_sentiment(park_tweet_text, control_tweet_text, stop_list,sample=n,runs=5)\n",
    "    print(\"{:.0f}% sampled: Mean Park: {:.4f} with range {:.4f}\".format(n*100, np.mean(bumps), max(bumps)-min(bumps)))\n",
    "#print(min(park_sentis), np.mean(park_sentis), max(park_sentis))\n",
    "#print(min(control_sentis), np.mean(control_sentis), max(control_sentis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT,labMTvector,labMTwordList = emotionFileReader(stopval=1.0,lang='english',returnVector=True)\n",
    "stop_lower = [x.lower() for x in dict(regex.get_ngrams(' '.join(stop_list), path='../src/func/ngrams.bin')).keys()]\n",
    "for word in stop_lower:\n",
    "    if word in labMTwordList:\n",
    "        i = labMTwordList.index(word)\n",
    "        val = labMTvector[i]\n",
    "        if val < 4 or val > 6:\n",
    "            pass\n",
    "            #print(word, labMTvector[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
