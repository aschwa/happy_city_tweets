{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "#from src.scripts.process_tweets import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a sample of tweets (park tweets, 2 control groups, park names)\n",
    "day = \"0601\"\n",
    "results_dir = Path(\"../data/processed/combined_tweets/tweet_samples_{}\".format(day))\n",
    "with open(results_dir/Path(\"park_tweets.json\"), 'r') as fp:\n",
    "    park_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"time_tweets.json\"), 'r') as fp:\n",
    "    time_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"user_tweets.json\"), 'r') as fp:\n",
    "    user_tweets = json.load(fp)\n",
    "with open(results_dir/Path(\"park_stops.json\"), 'r') as fp:\n",
    "    park_names = json.load(fp)\n",
    "\n",
    "# combine all tweets from all cities into lists of tweets\n",
    "park_tweet_text = []\n",
    "for city, tweets in park_tweets.items():\n",
    "    park_tweet_text += tweets\n",
    "time_tweet_text = []\n",
    "for city, tweets in time_tweets.items():\n",
    "    time_tweet_text += tweets\n",
    "user_tweet_text = []\n",
    "for city, tweets in user_tweets.items():\n",
    "      user_tweet_text += tweets\n",
    "park_stops = []\n",
    "for city, city_parks in park_names.items():\n",
    "    park_stops+=city_parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6973\n",
      "542\n"
     ]
    }
   ],
   "source": [
    "## load frequent stop words  stop words\n",
    "stop_file = Path(\"../src/sentiment/city_stops.json\")\n",
    "with open(stop_file, 'r') as fp:\n",
    "    stop_dict  = json.load(fp)\n",
    "    \n",
    "#load stop words selected from raw wordshifts for probelmatic words\n",
    "freq_stops = []\n",
    "for city, words in stop_dict.items():\n",
    "    freq_stops+=words\n",
    "freq_stops.append('park')\n",
    "freq_stops = list(set(freq_stops))\n",
    "\n",
    "# combine with  names of parks\n",
    "all_stops = freq_stops + park_stops\n",
    "\n",
    "\n",
    "# put through ngram filter, lower\n",
    "all_stops = list(set([x.lower() for x in dict(regex.get_ngrams(' '.join(all_stops), path='../src/func/ngrams.bin')).keys()]))\n",
    "print(len(all_stops))\n",
    "# select ones with valence below 4 or greater than 6\n",
    "stops_w_val=[]\n",
    "vals_of_stops = {}\n",
    "words = labmtgen.load_labmt_words(\"../data/raw/data_labmt_simple.txt\")\n",
    "senti_dict = words.set_index('Word').to_dict()['Happs']\n",
    "for stop in all_stops:\n",
    "    if stop in senti_dict.keys():\n",
    "        val = senti_dict[stop]\n",
    "        if val <= 4.0 or val >= 6.0:\n",
    "            #count+=1\n",
    "            stops_w_val.append(stop)\n",
    "        vals_of_stops[stop] = val\n",
    "print(len(stops_w_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/parkstop_vals.json\", 'w') as fp:\n",
    "    json.dump(vals_of_stops,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_sentis1, control_sentis1 = bootstrap_sentiment(park_tweet_text, time_tweet_text, all_stops,sample=.8,runs=10, bumps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap with various controls, stop combos\n",
    "park_sentis1, control_sentis1 = bootstrap_sentiment(park_tweet_text, control_tweet_text, all_stops,sample=.8,runs=10, bumps=False)\n",
    "park_sentis2, control_sentis2 = bootstrap_sentiment(park_tweet_text, control_tweet_text, freq_stops,sample=.8,runs=10, bumps=False)\n",
    "park_sentis3, control_sentis3 = bootstrap_sentiment(park_tweet_text, control_user_text, all_stops,sample=.8,runs=10, bumps=False)\n",
    "park_sentis4, control_sentis4 = bootstrap_sentiment(park_tweet_text, control_user_text, freq_stops,sample=.8,runs=10, bumps=False)\n",
    "#time_ctl, user_ctl = bootstrap_sentiment(park_tweet_txt, user_tweet_text, all_stops,sample=.8,runs=10, bumps=False)\n",
    "#results_dict[city.name]={\"park_sentiment\":park_sentis,\"control_sentiment\":control_sentis}#\n",
    "\n",
    "# save above\n",
    "results_dict = {\"park_sentiment\": park_sentis4, \"control_sentiment\":control_sentis4}\n",
    "#with open(\"./results/bootstrap_cities_freqstops_user.json\", 'w') as fp:\n",
    "#    json.dump(results_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sampled lists of tweets\n",
    "# go through city names\n",
    "park_tweet_samples = []\n",
    "time_tweet_samples = []\n",
    "user_tweet_samples = []\n",
    "#iterate through cities\n",
    "for city, tweets in park_tweets.items():\n",
    "    #list of indices to pull from each group\n",
    "    index_list = range(len(tweets))\n",
    "    # get random selection integers \n",
    "    sample = random.sample(index_list, 1000)\n",
    "    # add those to each output array\n",
    "    for i in sample:\n",
    "        park_tweet_samples.append(park_tweets[city][i])\n",
    "        time_tweet_samples.append(time_tweets[city][i])\n",
    "        user_tweet_samples.append(user_tweets[city][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate sentiment of sampled version\n",
    "park_sentis, control_sentis = bootstrap_sentiment(park_tweet_samples, time_tweet_samples, all_stops,sample=.8,runs=10, bumps=False)\n",
    "#park_sentis_u, control_sentis_u = bootstrap_sentiment(park_tweet_samples, user_tweet_samples, all_stops,sample=.8,runs=10, bumps=False)\n",
    "#park_sentis_t2, control_sentis_t2 = bootstrap_sentiment(park_tweet_samples, time_tweet_samples, freq_stops,sample=.8,runs=10, bumps=False)\n",
    "#park_sentis_u2, control_sentis_u2 = bootstrap_sentiment(park_tweet_samples, user_tweet_samples, freq_stops,sample=.8,runs=10, bumps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\"park_sentiment\": park_sentis1, \"control_sentiment\":control_sentis1}\n",
    "with open(\"./results/bootstrap_allcities_alltweets_allstops_time_0601.json\", 'w') as fp:\n",
    "    json.dump(results_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09205348420336268\n",
      "0.0851745898883971 0.09205348420336223 0.09812511365308296\n"
     ]
    }
   ],
   "source": [
    "with open('./results/bootstrap_allcities_allstops_time_0601.json','r') as fp:\n",
    "    results_dict = json.load(fp)\n",
    "    \n",
    "print(np.mean(results_dict['park_sentiment'])-np.mean(results_dict['control_sentiment']))\n",
    "sentis = []\n",
    "for i in range(10):\n",
    "    sentis.append(results_dict['park_sentiment'][i]-results_dict['control_sentiment'][i])\n",
    "print(np.min(sentis),np.mean(sentis),np.max(sentis))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
