{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T12:48:15.684959Z",
     "start_time": "2020-03-04T12:48:15.641891Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "from src.func import tweet_utils\n",
    "from src.func import regex\n",
    "from src.func import labmtgen\n",
    "from src.sentiment.senti_utils import *\n",
    "from labMTsimple.storyLab import *\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read excel file with word frequencies, manually labeled \"remove\" column\n",
    "list all remove words by city which are on separate sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('./results/city_words.xlsx')\n",
    "sheets =  pd.read_excel(xls, None)\n",
    "\n",
    "words_investigate = {}\n",
    "for city, df in sheets.items():\n",
    "    words_investigate[city] = list(df[df.remove ==1.0].word.values)\n",
    "\n",
    "#words_investigate\n",
    "#set(sum(words_investigate.values(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CA_San_Francisco_0667000': ['festival', 'music', 'young', 'flowers'],\n",
       " 'AZ_Phoenix_0455000': ['garden', 'hospital', 'zoo'],\n",
       " 'FL_Jacksonville_1235000': ['science', 'museum'],\n",
       " 'TX_Austin_4805000': ['music', 'limits', 'accident'],\n",
       " 'CA_San_Diego_0666000': ['zoo', 'sea'],\n",
       " 'DC_Washington_1150000': ['war', 'bill', 'united', 'health'],\n",
       " 'WA_Seattle_5363000': ['health', 'surgery', 'emergency'],\n",
       " 'IL_Chicago_1714000': ['museum', 'music', 'art', 'festival', 'riot'],\n",
       " 'TX_Houston_4835000': ['hospital',\n",
       "  'traffic',\n",
       "  'delay',\n",
       "  'zoo',\n",
       "  'accident',\n",
       "  'science',\n",
       "  'museum'],\n",
       " 'OH_Cleveland_3916000': ['garden', 'beach', 'island'],\n",
       " 'MA_Boston_2507000': ['closed', 'ma', 'partners', 'festival'],\n",
       " 'NY_New_York_3651000': ['festival', 'natural'],\n",
       " 'TX_San_Antonio_4865000': ['cafe', 'zoo'],\n",
       " 'TX_Dallas_4819000': ['festival', 'music', 'health'],\n",
       " 'PA_Philadelphia_4260000': ['independence', 'art'],\n",
       " 'CA_Los_Angeles_0644000': ['zoo', 'science', 'festival'],\n",
       " 'CA_San_Jose_0668000': ['christmas', 'raging'],\n",
       " 'CO_Denver_0820000': ['zoo', 'nature', 'science', 'museum', 'international'],\n",
       " 'TX_Fort_Worth_4827000': [],\n",
       " 'TX_El_Paso_4824000': ['music', 'festival'],\n",
       " 'TN_Memphis_4748000': ['steal', 'music', 'sugar', 'festival'],\n",
       " 'MI_Detroit_2622000': ['beach'],\n",
       " 'NC_Charlotte_3712000': ['shot', 'young', 'hiring'],\n",
       " 'IN_Indianapolis_1836003': ['health', 'accident'],\n",
       " 'OH_Columbus_3918000': ['roses', 'festival', 'gardens'],\n",
       " 'MD_Baltimore_2404000': ['closed', 'jobs']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stops = ['museum','art','festival','garden','gardens','zoo','closed','traffic','accident','hiring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['festival', 'music', 'young', 'flowers']\n"
     ]
    }
   ],
   "source": [
    "### Look at tweets with stop words for given city ###\n",
    "city = Path(\"../data/processed/tweets/CA_San_Francisco_0667000.json\")\n",
    "stops = words_investigate[city.stem]\n",
    "print(stops)\n",
    "tweets = load_tweets(city)\n",
    "park_tweets, control_tweets, park_names = park_control_split(tweets)\n",
    "user_tweets = user_control_split(tweets)\n",
    "\n",
    "count = 0 \n",
    "for user, tweets in user_tweets.items():\n",
    "    if count > 100:\n",
    "        break\n",
    "    for tweet in tweets:\n",
    "        try:\n",
    "            if  word in tweet['pure_text'].lower():\n",
    "                #print(word,tweet['pure_text'])\n",
    "                count+=1\n",
    "                break\n",
    "        except TypeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/processed/tweets/MI_Detroit_2622000.json'),\n",
       " PosixPath('../data/processed/tweets/IN_Indianapolis_1836003.json'),\n",
       " PosixPath('../data/processed/tweets/AZ_Phoenix_0455000.json'),\n",
       " PosixPath('../data/processed/tweets/DC_Washington_1150000.json'),\n",
       " PosixPath('../data/processed/tweets/TX_Houston_4835000.json')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Get hashtags for random set of cities ###\n",
    "\n",
    "city_list = []\n",
    "for city in tweet_dir.iterdir():\n",
    "    city_list.append(city)\n",
    "\n",
    "hash_cities = random.sample(city_list,5)\n",
    "\n",
    "park_hts = Counter()\n",
    "control_hts = Counter()\n",
    "\n",
    "for city in hash_cities:\n",
    "    city_words = []\n",
    "    print(\"City Hashtags {}\".format(city.name))\n",
    "    tweets = load_tweets(city)\n",
    "    park_tweets, control_tweets, park_names = park_control_split(tweets)\n",
    "    park_text = ' '.join([str(tweet['pure_text']).lower() for tweet in park_tweets])\n",
    "    park_ngrams = dict(regex.get_ngrams(park_text,path='../src/func/ngrams.bin'))\n",
    "    control_text = ' '.join([str(tweet['pure_text']).lower() for tweet in control_tweets])\n",
    "    control_ngrams = dict(regex.get_ngrams(control_text,path='../src/func/ngrams.bin'))\n",
    "    park_hashtags = {}\n",
    "\n",
    "    for k,v in park_ngrams.items():\n",
    "        if k[0]==\"#\" and v > 50:\n",
    "            park_hashtags[k] = v\n",
    "\n",
    "    park_hts += Counter(park_hashtags)\n",
    "\n",
    "    control_hashtags = {}\n",
    "    for k,v in control_ngrams.items():\n",
    "        if k[0]==\"#\" and v > 50:\n",
    "            control_hashtags[k] = v\n",
    "\n",
    "    control_hts += Counter(control_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#job', 6223),\n",
       " ('#trndnl', 4054),\n",
       " ('#hiring', 4027),\n",
       " ('#jobs', 3471),\n",
       " ('#houston', 2229),\n",
       " ('#careerarc', 1758),\n",
       " ('#indianapolis', 1609),\n",
       " ('#washington', 1480),\n",
       " ('#tweetmyjobs', 1047),\n",
       " ('#hospitality', 840),\n",
       " ('#dc', 818),\n",
       " ('#traffic', 724),\n",
       " ('#phoenix', 718),\n",
       " ('#nursing', 709),\n",
       " ('#veterans', 616),\n",
       " ('#retail', 545),\n",
       " ('#healthcare', 534),\n",
       " ('#top3apps', 387),\n",
       " ('#washingtondc', 378),\n",
       " ('#sales', 326),\n",
       " ('#', 292),\n",
       " ('#it', 209),\n",
       " ('#clerical', 198),\n",
       " ('#internship', 182),\n",
       " ('#detroit', 181),\n",
       " ('#nats', 171),\n",
       " ('#businessmgmt', 154),\n",
       " ('#photo', 148),\n",
       " ('#customerservice', 129),\n",
       " ('#accounting', 126)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_hts.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hashtags to remove\n",
    "hashtags_rm = [\"#jobs\",\"#job\", \"#getalljobs\",\"#hiring\", \"#tweetmyjobs\", \"#careerarc\",\"#hospitality\", \"#healthcare\", \"#nursing\",\"#marketing\", \"#sales\",\"#clerical\",\"#it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/sentiment/hashtags.json\", 'w') as fp:\n",
    "    json.dump(hashtags_rm, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
