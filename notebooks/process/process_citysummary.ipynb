{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-03T21:16:17.530664Z",
     "start_time": "2020-03-03T21:16:17.491992Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T12:48:15.684959Z",
     "start_time": "2020-03-04T12:48:15.641891Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530/\")\n",
    "#city = Path(\"../data/processed/tweets/CO_Denver_0820000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets every tweets is labeled with a park name or nan\n",
    "def build_info_table(tweet_dir, old_index_values = []):\n",
    "    city_info = []\n",
    "    for city in tweet_dir.glob(\"*.json\"):\n",
    "        city_name = city.stem\n",
    "        #city_summary.index.values \n",
    "        if city_name not in old_index_values:\n",
    "            print(\"{}\".format(city_name))\n",
    "            with open(city, 'r') as f:\n",
    "                park_user_tweets = json.load(f)\n",
    "            n_park_users = len(park_user_tweets)\n",
    "            total_tweets = 0\n",
    "            park_tweets = 0\n",
    "            parks_visisted = []\n",
    "            for user_tweets in park_user_tweets.values():\n",
    "                total_tweets += len(user_tweets)\n",
    "                for tweet in user_tweets:\n",
    "                    if not pd.isnull(tweet['ParkID']):\n",
    "                        park_tweets+=1\n",
    "                        parks_visited.append(tweet['Park_Name'])\n",
    "            users = list(set([tweet['user']['id_str'] for tweet in tweets]))\n",
    "            n_users = len(users)\n",
    "            n_park_tweets = len(park_tweets)\n",
    "            n_parks_w_tweets = len(set(park_names))\n",
    "            park_users = list(set([tweet['user']['id_str'] for tweet in park_tweets]))\n",
    "            n_park_users = len(park_users)\n",
    "            summary = {\"city_file\":city,\n",
    "                       \"city\": city_name,\n",
    "                       \"tweets\":n_tweets,\n",
    "                       \"users\":n_users,\n",
    "                       \"park_tweets\":n_park_tweets,\n",
    "                       \"park_users\":n_park_users,\n",
    "                       \"parks_visited\":n_parks_w_tweets}\n",
    "            city_info.append(summary)\n",
    "    return city_table\n",
    "\n",
    "def process_city_table(new_table, old_table):\n",
    "    new_table = pd.DataFrame.from_records(new_table, index = 'city').sort_values('tweets',ascending=False)\n",
    "    city_table = pd.concat([old_table, new_table])\n",
    "    #city_table.to_csv('./results/city_summary.csv')\n",
    "    return city_table\n",
    "\n",
    "def build_user_table(tweet_dir):\n",
    "    city_info = {}\n",
    "    for city in tweet_dir.glob(\"*.json\"):\n",
    "        city_name = city.stem\n",
    "        print(\"{}\".format(city_name))\n",
    "        with open(city, 'r') as f:\n",
    "            park_user_tweets = json.load(f)\n",
    "        n_park_users = len(park_user_tweets)\n",
    "        total_tweets = 0\n",
    "        park_tweets = 0\n",
    "        parks_visited = []\n",
    "        for user_tweets in park_user_tweets.values():\n",
    "            total_tweets += len(user_tweets)\n",
    "            for tweet in user_tweets:\n",
    "                if not pd.isnull(tweet['ParkID']):\n",
    "                    park_tweets+=1\n",
    "                    parks_visited.append(tweet['Park_Name'])\n",
    "        n_parks_visisted = len(set(parks_visited))\n",
    "        city_info[city_name] = {\"Park Visitors\":n_park_users,\"Total Tweets\":total_tweets,\"Park Tweets\":park_tweets,\"Parks Visited\":n_parks_visisted}\n",
    "    return city_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dir= Path(\"../data/processed/park_user_tweets_0530/\")\n",
    "city_info = build_user_table(tweet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Total Tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dd3de2f95636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Total Tweets'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Park Tweets'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'% Tweets in Park'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Park Visitors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Parks Visited'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Total Tweets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'% Tweets in Park'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Park Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   5296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5297\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5298\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5300\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Total Tweets'"
     ]
    }
   ],
   "source": [
    "order = ['Total Tweets','Park Tweets','% Tweets in Park','Park Visitors', 'Parks Visited']\n",
    "df = pd.DataFrame.from_dict(city_info, orient='index').sort_values(by='Total Tweets', ascending=False)\n",
    "df['% Tweets in Park'] = np.round(df['Park Tweets']/df['Total Tweets'],3)*100\n",
    "df[order].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city'] = [x[3:-8].replace(\"_\",\" \") for x in df.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.read_csv(\"../data/processed/city_info.csv\")\n",
    "pop.rename({\"NAME\":\"city\"}, axis=1, inplace=True)\n",
    "summary = pd.merge(df,pop[['city','POP2012']],on='city').set_index('city')\n",
    "summary['Tweets per capita'] = np.round(summary['Total Tweets'] / summary['POP2012'],2)\n",
    "summary = summary.drop('POP2012',axis=1)\n",
    "order = ['Total Tweets','Park Tweets','% Tweets in Park','Park Visitors', 'Parks Visited', 'Tweets per capita']\n",
    "summary['Park Visitors'] = summary['Park Visitors'].apply(lambda x : \"{:,}\".format(x))\n",
    "summary['Total Tweets'] = summary['Total Tweets'].apply(lambda x : \"{:,}\".format(x))\n",
    "summary['Parks Visited'] = summary['Parks Visited'].apply(lambda x : \"{:,}\".format(x))\n",
    "summary['Park Tweets'] = summary['Park Tweets'].apply(lambda x : \"{:,}\".format(x))\n",
    "summary = summary[order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrllr}\n",
      "\\toprule\n",
      "{} & Total Tweets & Park Tweets &  \\% Tweets in Park & Park Visitors & Parks Visited &  Tweets per capita \\\\\n",
      "city          &              &             &                   &               &               &                    \\\\\n",
      "\\midrule\n",
      "New York      &    2,892,512 &     213,813 &               7.4 &       113,702 &         1,880 &               0.35 \\\\\n",
      "Los Angeles   &    1,215,288 &      53,988 &               4.4 &        36,271 &           540 &               0.32 \\\\\n",
      "Philadelphia  &    1,166,125 &      64,857 &               5.6 &        26,287 &           482 &               0.76 \\\\\n",
      "Chicago       &    1,130,611 &      66,100 &               5.8 &        36,919 &           872 &               0.41 \\\\\n",
      "Houston       &      821,433 &      39,581 &               4.8 &        13,464 &           501 &               0.38 \\\\\n",
      "San Antonio   &      589,595 &      23,566 &               4.0 &        12,763 &           268 &               0.43 \\\\\n",
      "Washington    &      570,157 &      74,937 &              13.1 &        41,062 &           370 &               0.92 \\\\\n",
      "Boston        &      547,625 &      52,689 &               9.6 &        23,479 &           682 &               0.87 \\\\\n",
      "San Diego     &      491,219 &      36,080 &               7.3 &        22,269 &           406 &               0.37 \\\\\n",
      "Dallas        &      490,918 &      21,787 &               4.4 &        12,211 &           346 &               0.40 \\\\\n",
      "San Francisco &      486,782 &      59,412 &              12.2 &        36,175 &           407 &               0.59 \\\\\n",
      "Austin        &      449,853 &      23,547 &               5.2 &        14,689 &           289 &               0.55 \\\\\n",
      "Baltimore     &      333,734 &      12,965 &               3.9 &         5,135 &           260 &               0.53 \\\\\n",
      "Fort Worth    &      320,178 &       9,664 &               3.0 &         4,278 &           239 &               0.42 \\\\\n",
      "Phoenix       &      268,455 &      12,041 &               4.5 &         7,566 &           189 &               0.18 \\\\\n",
      "Columbus      &      251,573 &       8,884 &               3.5 &         4,340 &           328 &               0.31 \\\\\n",
      "San Jose      &      234,234 &       8,263 &               3.5 &         4,517 &           314 &               0.24 \\\\\n",
      "Indianapolis  &      225,931 &      11,560 &               5.1 &         5,660 &           183 &               0.27 \\\\\n",
      "Charlotte     &      218,310 &       8,039 &               3.7 &         3,868 &           190 &               0.29 \\\\\n",
      "Seattle       &      201,533 &      12,758 &               6.3 &         7,739 &           373 &               0.32 \\\\\n",
      "Detroit       &      195,572 &       7,885 &               4.0 &         3,819 &           234 &               0.28 \\\\\n",
      "Jacksonville  &      194,777 &       6,219 &               3.2 &         3,218 &           261 &               0.23 \\\\\n",
      "Memphis       &      137,222 &       5,614 &               4.1 &         3,112 &           163 &               0.21 \\\\\n",
      "Denver        &      131,240 &       6,243 &               4.8 &         3,902 &           279 &               0.21 \\\\\n",
      "El Paso       &       96,015 &       2,722 &               2.8 &         1,397 &           180 &               0.14 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv('./results/summary_tables/summary_0605.csv')\n",
    "#df.to_html('table.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
